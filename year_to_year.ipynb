{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#access the csv called batting_season_summary.csv in example_data:\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('example_data/batting_season_summary.csv')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#alrigth so we have the columns of years\n",
    "# i want to rearrange it, such that each row has the player with a data for year x, and then the data for year x+1\n",
    "#for example, if there used to be two rows like:\n",
    "# player1, data1, data2, data3, 2014\n",
    "# player1, data4, data5, data6, 2015\n",
    "#player1, data8, data9, data10, 2016\n",
    "#then this should make 2 rows like:\n",
    "#player1, data1, data2, data3, 2014, data4, data5, data6, 2015\n",
    "#player1, data4, data5, data6, 2015, data8, data9, data10, 2016\n",
    "#anyways, for a particular player who has entries for n years, there should be n-1 rows for that player in the new dataframe.\n",
    "#heres the code:\n",
    "\n",
    "#no more comments, heres the code:\n",
    "\n",
    "#sort the df by player and year\n",
    "df = df.sort_values(by=['Name', 'Year'])\n",
    "#drop the columns 'team' and 'pos' \n",
    "df = df.drop(columns=['team', 'pos'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2700, 16)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss: 26542.6875\n",
      "epoch 2, loss: 20002.330078125\n",
      "epoch 3, loss: 17104.216796875\n",
      "epoch 4, loss: 7737.54833984375\n",
      "epoch 5, loss: 3047.875732421875\n",
      "epoch 6, loss: 2668.345947265625\n",
      "epoch 7, loss: 2135.83154296875\n",
      "epoch 8, loss: 1949.6180419921875\n",
      "epoch 9, loss: 1670.4635009765625\n",
      "epoch 10, loss: 999.3551635742188\n",
      "epoch 11, loss: 1261.3460693359375\n",
      "epoch 12, loss: 1081.0579833984375\n",
      "epoch 13, loss: 683.124755859375\n",
      "epoch 14, loss: 757.15234375\n",
      "epoch 15, loss: 798.0426635742188\n",
      "epoch 16, loss: 473.7430419921875\n",
      "epoch 17, loss: 562.0117797851562\n",
      "epoch 18, loss: 484.504638671875\n",
      "epoch 19, loss: 418.5822448730469\n",
      "epoch 20, loss: 352.607666015625\n",
      "epoch 21, loss: 307.84930419921875\n",
      "epoch 22, loss: 534.7980346679688\n",
      "epoch 23, loss: 288.90997314453125\n",
      "epoch 24, loss: 227.56394958496094\n",
      "epoch 25, loss: 184.62200927734375\n",
      "epoch 26, loss: 192.52256774902344\n",
      "epoch 27, loss: 102.4846420288086\n",
      "epoch 28, loss: 117.33736419677734\n",
      "epoch 29, loss: 113.6229248046875\n",
      "epoch 30, loss: 95.54291534423828\n",
      "epoch 31, loss: 78.1455307006836\n",
      "epoch 32, loss: 83.4542007446289\n",
      "epoch 33, loss: 61.404483795166016\n",
      "epoch 34, loss: 38.51791763305664\n",
      "epoch 35, loss: 51.98221969604492\n",
      "epoch 36, loss: 23.039403915405273\n",
      "epoch 37, loss: 27.367753982543945\n",
      "epoch 38, loss: 29.66045379638672\n",
      "epoch 39, loss: 20.398195266723633\n",
      "epoch 40, loss: 27.911672592163086\n",
      "epoch 41, loss: 18.722728729248047\n",
      "epoch 42, loss: 23.04529571533203\n",
      "epoch 43, loss: 11.10903263092041\n",
      "epoch 44, loss: 10.10716724395752\n",
      "epoch 45, loss: 9.85430908203125\n",
      "epoch 46, loss: 9.498262405395508\n",
      "epoch 47, loss: 11.640037536621094\n",
      "epoch 48, loss: 7.234426975250244\n",
      "epoch 49, loss: 11.65857982635498\n",
      "epoch 50, loss: 7.885725498199463\n",
      "epoch 51, loss: 5.23107385635376\n",
      "epoch 52, loss: 7.838160037994385\n",
      "epoch 53, loss: 7.616708278656006\n",
      "epoch 54, loss: 4.632968425750732\n",
      "epoch 55, loss: 4.642829895019531\n",
      "epoch 56, loss: 3.282402992248535\n",
      "epoch 57, loss: 3.4180843830108643\n",
      "epoch 58, loss: 2.407383918762207\n",
      "epoch 59, loss: 3.567957878112793\n",
      "epoch 60, loss: 5.0398478507995605\n",
      "epoch 61, loss: 3.343388795852661\n",
      "epoch 62, loss: 4.202654838562012\n",
      "epoch 63, loss: 3.4617748260498047\n",
      "epoch 64, loss: 3.262465238571167\n",
      "epoch 65, loss: 2.870694160461426\n",
      "epoch 66, loss: 2.904369354248047\n",
      "epoch 67, loss: 2.2782866954803467\n",
      "epoch 68, loss: 3.3347063064575195\n",
      "epoch 69, loss: 1.9524298906326294\n",
      "epoch 70, loss: 4.076673984527588\n",
      "epoch 71, loss: 2.8147823810577393\n",
      "epoch 72, loss: 4.113796234130859\n",
      "epoch 73, loss: 1.6338266134262085\n",
      "epoch 74, loss: 3.119847536087036\n",
      "epoch 75, loss: 1.5312894582748413\n",
      "epoch 76, loss: 2.718191385269165\n",
      "epoch 77, loss: 1.646399974822998\n",
      "epoch 78, loss: 1.926332950592041\n",
      "epoch 79, loss: 1.514414668083191\n",
      "epoch 80, loss: 1.2894048690795898\n",
      "epoch 81, loss: 3.0312998294830322\n",
      "epoch 82, loss: 3.1577072143554688\n",
      "epoch 83, loss: 1.0823489427566528\n",
      "epoch 84, loss: 2.0418617725372314\n",
      "epoch 85, loss: 2.257326602935791\n",
      "epoch 86, loss: 1.422402262687683\n",
      "epoch 87, loss: 1.2759180068969727\n",
      "epoch 88, loss: 1.4387969970703125\n",
      "epoch 89, loss: 1.4697657823562622\n",
      "epoch 90, loss: 1.3017715215682983\n",
      "epoch 91, loss: 1.1208217144012451\n",
      "epoch 92, loss: 1.0922402143478394\n",
      "epoch 93, loss: 1.3384919166564941\n",
      "epoch 94, loss: 1.2864065170288086\n",
      "epoch 95, loss: 0.7326352596282959\n",
      "epoch 96, loss: 0.5961626172065735\n",
      "epoch 97, loss: 0.8819822669029236\n",
      "epoch 98, loss: 1.4925671815872192\n",
      "epoch 99, loss: 1.3350672721862793\n",
      "epoch 100, loss: 1.1684187650680542\n",
      "test loss: 1.096006155014038\n",
      "test mse: 1.096006155014038\n"
     ]
    }
   ],
   "source": [
    "#alright do a neural network taking all this data (except name and year) and predict H. use pytorch:\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "#for reference theres 14 columns in the df, 1 of them is the target (H). everything has been dropped so just use. heres the code:\n",
    "\n",
    "#split the data into train and test\n",
    "X = df.drop(columns=['H', 'BA'] ).values\n",
    "y = df['H'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#scale the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "#convert the data to tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "#make a dataloader\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "#make the neural network\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(X.shape[1], 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "model = Net()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "#train the model\n",
    "for epoch in range(100):\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_batch)\n",
    "        loss = criterion(output, y_batch.view(-1, 1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'epoch {epoch+1}, loss: {loss.item()}')\n",
    "\n",
    "#evaluate the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X_test)\n",
    "    loss = criterion(y_pred, y_test.view(-1, 1))\n",
    "    print(f'test loss: {loss.item()}')\n",
    "    print(f'test mse: {mean_squared_error(y_test, y_pred)}')\n",
    "\n",
    "#save the model\n",
    "torch.save(model.state_dict(), 'model.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9987925900738972\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 8, 'n_estimators': 70}\n",
      "0.9669703256409132\n",
      "36.785861681433786\n",
      "0.9594750342526668\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from math import radians, sin, cos, sqrt, atan2\n",
    "\n",
    "# Function to calculate distance between two geographical coordinates using Haversine formula\n",
    "def calculate_distance(lat1, lon1, lat2, lon2):\n",
    "    # Radius of the Earth in km\n",
    "    R = 6371.0\n",
    "    \n",
    "    # Convert latitude and longitude from degrees to radians\n",
    "    lat1 = radians(lat1)\n",
    "    lon1 = radians(lon1)\n",
    "    lat2 = radians(lat2)\n",
    "    lon2 = radians(lon2)\n",
    "    \n",
    "    # Calculate the change in coordinates\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "    \n",
    "    # Apply Haversine formula\n",
    "    a = sin(dlat / 2)**2 + cos(lat1) * cos(lat2) * sin(dlon / 2)**2\n",
    "    c = 2 * atan2(sqrt(a), sqrt(1 - a))\n",
    "    distance = R * c\n",
    "    \n",
    "    return distance\n",
    "\n",
    "# Function to get average distance traveled for a given year and team\n",
    "def get_average_distance(year, team):\n",
    "    # Load schedule data for the given year\n",
    "    schedule_filename = f'example_data/schedules/{year}_schedule.csv'\n",
    "    total_distance = 0\n",
    "    games_played = 0\n",
    "    \n",
    "    # Parse the schedule and calculate distances\n",
    "    with open(schedule_filename, 'r') as file:\n",
    "        next(file)  # Skip header\n",
    "        for line in file:\n",
    "            schedule_year, _, home_team, away_team = line.strip().split(',')[0:4]\n",
    "            \n",
    "            # Check if it's an away game for the given team\n",
    "            if schedule_year == year and away_team == team:\n",
    "                # Check if coordinates for both cities are available\n",
    "                if home_team in city_coordinates and away_team in city_coordinates:\n",
    "                    home_lat, home_lon = city_coordinates[home_team]\n",
    "                    away_lat, away_lon = city_coordinates[away_team]\n",
    "                    \n",
    "                    # Calculate distance between home and away cities\n",
    "                    distance = calculate_distance(home_lat, home_lon, away_lat, away_lon)\n",
    "                    total_distance += distance\n",
    "                    games_played += 1\n",
    "    \n",
    "    # Calculate average distance traveled\n",
    "    if games_played > 0:\n",
    "        return total_distance / games_played\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Dictionary to store the coordinates of each city\n",
    "city_coordinates = {\n",
    "    'Royals': (39.0513, -94.4805),     # Kansas City (Royals)\n",
    "    'Braves': (33.8908, -84.4679),     # Atlanta (Braves)\n",
    "    'Rays': (27.7684, -82.6483),       # St. Petersburg (Rays)\n",
    "    'Blue Jays': (43.6414, -79.3894),  # Toronto (Blue Jays)\n",
    "    'Diamondbacks': (33.4455, -112.0667),  # Phoenix (Diamondbacks)\n",
    "    'Astros': (29.7572, -95.3554),     # Houston (Astros)\n",
    "    'Pirates': (40.4469, -80.0057),    # Pittsburgh (Pirates)\n",
    "    'Dodgers': (34.0736, -118.2400),   # Los Angeles (Dodgers)\n",
    "    'Rockies': (39.7554, -104.9881),   # Denver (Rockies)\n",
    "    'Nationals': (38.8729, -77.0074),  # Washington D.C. (Nationals)\n",
    "    'Cardinals': (38.6226, -90.1928),  # St. Louis (Cardinals)\n",
    "    'Red Sox': (42.3467, -71.0972),    # Boston (Red Sox)\n",
    "    'Orioles': (39.2839, -76.6217),    # Baltimore (Orioles)\n",
    "    'Giants': (37.7786, -122.3893),    # San Francisco (Giants)\n",
    "    'Reds': (39.0979, -84.5086),       # Cincinnati (Reds)\n",
    "    'Indians': (41.4959, -81.6853),    # Cleveland (Indians)\n",
    "    'Padres': (32.7076, -117.1570),    # San Diego (Padres)\n",
    "    'Phillies': (39.9054, -75.1669),   # Philadelphia (Phillies)\n",
    "    'White Sox': (41.8301, -87.6347),  # Chicago (White Sox)\n",
    "    'Brewers': (43.0280, -87.9712),    # Milwaukee (Brewers)\n",
    "    'Yankees': (40.8296, -73.9262),    # New York City (Yankees)\n",
    "    'Mets': (40.7571, -73.8458),       # New York City (Mets)\n",
    "    'Rangers': (32.7511, -97.0820),    # Arlington (Rangers)\n",
    "    'Marlins': (25.7780, -80.2195),    # Miami (Marlins)\n",
    "    'Mariners': (47.5914, -122.3325),  # Seattle (Mariners)\n",
    "    'Twins': (44.9817, -93.2784),      # Minneapolis (Twins)\n",
    "    'Angels': (33.8003, -117.8827),    # Anaheim (Angels)\n",
    "    'Cubs': (41.9484, -87.6553),       # Chicago (Cubs)\n",
    "    'Athletics': (37.7516, -122.2005), # Oakland (Athletics)\n",
    "    'Tigers': (42.3391, -83.0487)      # Detroit (Tigers\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Name  age_2022  age_2023     team_2022     team_2023  \\\n",
      "0         Aaron Looper        37        38  Diamondbacks  Diamondbacks   \n",
      "1       Aaron Scheffer        38        39        Astros        Astros   \n",
      "2        Adam Peterson        29        30        Braves        Braves   \n",
      "3            Adam Wilk        20        21       Rangers       Rangers   \n",
      "4        Adrian Houser        41        42        Royals        Royals   \n",
      "..                 ...       ...       ...           ...           ...   \n",
      "232    Tyler Ladendorf        29        30       Rangers       Rangers   \n",
      "233      Victor Garate        28        29      Phillies      Phillies   \n",
      "234      Vince Belnome        30        31        Giants        Giants   \n",
      "235     Wilfredo Tovar        26        27      Mariners      Mariners   \n",
      "236  Wilking Rodriguez        33        34         Twins         Twins   \n",
      "\n",
      "    pos_2022 pos_2023  PA_2022  PA_2023  AB_2022  ...   BA_2022   BA_2023  \\\n",
      "0          C        C      723      723      655  ...  0.248855  0.244168   \n",
      "1          C        C      736      745      676  ...  0.278107  0.247788   \n",
      "2         1B       1B      723      711      649  ...  0.257319  0.248466   \n",
      "3         SS       SS      682      675      655  ...  0.203053  0.247295   \n",
      "4         SS       SS      645      644      598  ...  0.209030  0.238806   \n",
      "..       ...      ...      ...      ...      ...  ...       ...       ...   \n",
      "232       1B       1B      740      735      655  ...  0.212214  0.197836   \n",
      "233       RF       RF      667      646      604  ...  0.301325  0.278351   \n",
      "234       SS       SS      695      710      647  ...  0.258114  0.262295   \n",
      "235       1B       1B      753      756      667  ...  0.208396  0.218425   \n",
      "236       2B       2B      713      707      635  ...  0.247244  0.237578   \n",
      "\n",
      "     OBP_2022  OBP_2023  SLG_2022  SLG_2023  OPS_2022  OPS_2023  \\\n",
      "0    0.319502  0.327801  0.421374  0.419907  0.740876  0.747708   \n",
      "1    0.336957  0.315436  0.500000  0.412979  0.836957  0.728416   \n",
      "2    0.333333  0.310830  0.432974  0.457055  0.766307  0.767885   \n",
      "3    0.234604  0.278519  0.358779  0.403400  0.593383  0.681919   \n",
      "4    0.266667  0.287267  0.364548  0.426202  0.631215  0.713469   \n",
      "..        ...       ...       ...       ...       ...       ...   \n",
      "232  0.302703  0.293878  0.329771  0.319938  0.632474  0.613816   \n",
      "233  0.367316  0.349845  0.504967  0.486254  0.872283  0.836099   \n",
      "234  0.309353  0.302817  0.442040  0.469449  0.751393  0.772265   \n",
      "235  0.298805  0.304233  0.332834  0.349183  0.631638  0.653416   \n",
      "236  0.329593  0.305516  0.409449  0.414596  0.739042  0.720113   \n",
      "\n",
      "     average_distance_traveled_2022  average_distance_traveled_2023  \n",
      "0                       2225.270942                     2179.360569  \n",
      "1                       1804.150154                     1851.571809  \n",
      "2                       1579.628997                     1555.648618  \n",
      "3                       1670.087164                     1653.825187  \n",
      "4                       1392.793060                     1393.296270  \n",
      "..                              ...                             ...  \n",
      "232                     1670.087164                     1653.825187  \n",
      "233                     1666.534873                     1760.432996  \n",
      "234                     2917.531625                     2737.547001  \n",
      "235                     2815.169960                     2918.115753  \n",
      "236                     1625.335139                     1584.117562  \n",
      "\n",
      "[237 rows x 35 columns]\n"
     ]
    }
   ],
   "source": [
    "#ok this is what we're going to do. look ath the file batting_season_summary.\n",
    "# Name,age,team,pos,PA,AB,H,2B,3B,HR,BB,SO,P/PA,BA,OBP,SLG,OPS,Year.\n",
    "\n",
    "#train a neural networking taking age, team, pos, PA, AB, 2B, 3B, HR, BB, SO, P/PA, BA, OBP, SLG, OPS, SPECIFICALLY FOR YEAR 2022 as input,\n",
    "#and outputs age, team, pos, PA, AB, H, 2B, 3B, HR, BB, SO, P/PA, BA, OBP, SLG, OPS for year 2023.\n",
    "#basically each player who has had an entry in both 2022 and 2023 will be sort of used as data, where the 2022 one is their input and the 2023 one is their output to compare against.\n",
    "#so let's make the model:\n",
    "\n",
    "#first we need to get the data for 2022 and 2023\n",
    "df = pd.read_csv('example_data/batting_season_summary.csv')\n",
    "df['average_distance_traveled'] = df.apply(lambda row: get_average_distance(str(row['Year']), row['team']), axis=1)\n",
    "\n",
    "#drop the rows that don't have 2022 or 2023\n",
    "df = df[(df['Year'] == 2022) | (df['Year'] == 2023)]\n",
    "#sort by name:\n",
    "df = df.sort_values(by='Name')\n",
    "\n",
    "#drop names that only have 1 entry\n",
    "df = df.groupby('Name').filter(lambda x: len(x) == 2)\n",
    "\n",
    "#now basically combine the 2022 and 2023 data for each player into one row:\n",
    "df = df.pivot(index='Name', columns='Year')\n",
    "df.columns = ['_'.join(map(str, col)).strip() for col in df.columns.values]\n",
    "df = df.reset_index()\n",
    "print(df) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss: 1.038475513458252\n",
      "epoch 2, loss: 0.7906256318092346\n",
      "epoch 3, loss: 0.8985239863395691\n",
      "epoch 4, loss: 1.024081826210022\n",
      "epoch 5, loss: 1.0124821662902832\n",
      "epoch 6, loss: 0.8748965263366699\n",
      "epoch 7, loss: 0.7265164852142334\n",
      "epoch 8, loss: 0.9148544073104858\n",
      "epoch 9, loss: 0.9607894420623779\n",
      "epoch 10, loss: 0.7959932088851929\n",
      "epoch 11, loss: 1.0125590562820435\n",
      "epoch 12, loss: 0.7897387742996216\n",
      "epoch 13, loss: 0.7406659722328186\n",
      "epoch 14, loss: 0.6278113722801208\n",
      "epoch 15, loss: 0.5741518139839172\n",
      "epoch 16, loss: 0.5436609387397766\n",
      "epoch 17, loss: 0.6064728498458862\n",
      "epoch 18, loss: 0.590688943862915\n",
      "epoch 19, loss: 0.629690945148468\n",
      "epoch 20, loss: 0.5396530628204346\n",
      "epoch 21, loss: 0.7049775123596191\n",
      "epoch 22, loss: 0.6235377788543701\n",
      "epoch 23, loss: 0.6421233415603638\n",
      "epoch 24, loss: 0.6262005567550659\n",
      "epoch 25, loss: 0.5907728672027588\n",
      "epoch 26, loss: 0.6454694867134094\n",
      "epoch 27, loss: 0.5638565421104431\n",
      "epoch 28, loss: 0.5927125215530396\n",
      "epoch 29, loss: 0.5657119750976562\n",
      "epoch 30, loss: 0.44162657856941223\n",
      "epoch 31, loss: 0.45590656995773315\n",
      "epoch 32, loss: 0.631210446357727\n",
      "epoch 33, loss: 0.3773721754550934\n",
      "epoch 34, loss: 0.46690869331359863\n",
      "epoch 35, loss: 0.3782481849193573\n",
      "epoch 36, loss: 0.34355729818344116\n",
      "epoch 37, loss: 0.42494890093803406\n",
      "epoch 38, loss: 0.26595252752304077\n",
      "epoch 39, loss: 0.47287517786026\n",
      "epoch 40, loss: 0.39244186878204346\n",
      "epoch 41, loss: 0.3002696931362152\n",
      "epoch 42, loss: 0.40572407841682434\n",
      "epoch 43, loss: 0.41830214858055115\n",
      "epoch 44, loss: 0.3428742289543152\n",
      "epoch 45, loss: 0.37530291080474854\n",
      "epoch 46, loss: 0.2902512550354004\n",
      "epoch 47, loss: 0.32580360770225525\n",
      "epoch 48, loss: 0.3739667236804962\n",
      "epoch 49, loss: 0.2534286081790924\n",
      "epoch 50, loss: 0.2721479535102844\n",
      "epoch 51, loss: 0.2968851327896118\n",
      "epoch 52, loss: 0.27278774976730347\n",
      "epoch 53, loss: 0.3002825379371643\n",
      "epoch 54, loss: 0.2532026469707489\n",
      "epoch 55, loss: 0.3468557894229889\n",
      "epoch 56, loss: 0.31220123171806335\n",
      "epoch 57, loss: 0.2501275837421417\n",
      "epoch 58, loss: 0.2977184057235718\n",
      "epoch 59, loss: 0.32725754380226135\n",
      "epoch 60, loss: 0.25371068716049194\n",
      "epoch 61, loss: 0.25435304641723633\n",
      "epoch 62, loss: 0.2748173475265503\n",
      "epoch 63, loss: 0.264570027589798\n",
      "epoch 64, loss: 0.26340970396995544\n",
      "epoch 65, loss: 0.21926122903823853\n",
      "epoch 66, loss: 0.25863179564476013\n",
      "epoch 67, loss: 0.27453184127807617\n",
      "epoch 68, loss: 0.2557432949542999\n",
      "epoch 69, loss: 0.25198838114738464\n",
      "epoch 70, loss: 0.26390671730041504\n",
      "epoch 71, loss: 0.2434656322002411\n",
      "epoch 72, loss: 0.2651907801628113\n",
      "epoch 73, loss: 0.2925484776496887\n",
      "epoch 74, loss: 0.23382841050624847\n",
      "epoch 75, loss: 0.22305214405059814\n",
      "epoch 76, loss: 0.20654796063899994\n",
      "epoch 77, loss: 0.23828795552253723\n",
      "epoch 78, loss: 0.20159463584423065\n",
      "epoch 79, loss: 0.179696723818779\n",
      "epoch 80, loss: 0.20450377464294434\n",
      "epoch 81, loss: 0.2688153386116028\n",
      "epoch 82, loss: 0.21524839103221893\n",
      "epoch 83, loss: 0.23140838742256165\n",
      "epoch 84, loss: 0.24885790050029755\n",
      "epoch 85, loss: 0.25417494773864746\n",
      "epoch 86, loss: 0.25412803888320923\n",
      "epoch 87, loss: 0.2916480004787445\n",
      "epoch 88, loss: 0.2525099217891693\n",
      "epoch 89, loss: 0.280291885137558\n",
      "epoch 90, loss: 0.23623767495155334\n",
      "epoch 91, loss: 0.2187117338180542\n",
      "epoch 92, loss: 0.19152897596359253\n",
      "epoch 93, loss: 0.2904469966888428\n",
      "epoch 94, loss: 0.2400589883327484\n",
      "epoch 95, loss: 0.23467084765434265\n",
      "epoch 96, loss: 0.2695993483066559\n",
      "epoch 97, loss: 0.19063401222229004\n",
      "epoch 98, loss: 0.2752994894981384\n",
      "epoch 99, loss: 0.2007567584514618\n",
      "epoch 100, loss: 0.20538882911205292\n",
      "test loss: 0.21070241928100586\n",
      "test mse: 0.21070246398448944\n",
      "0.7621664183899095\n"
     ]
    }
   ],
   "source": [
    "#ok now we train a neural network to predict the 2023 data from the 2022 data:\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "#the input is the 2022 data, the output is the 2023 data.\n",
    "#aka the input is age_2022, team_2022, pos_2022, PA_2022, AB_2022, H_2022, 2B_2022, 3B_2022, HR_2022, BB_2022, SO_2022, P/PA_2022, BA_2022, OBP_2022, SLG_2022, OPS_2022\n",
    "#the output is age_2023, team_2023, pos_2023, PA_2023, AB_2023, H_2023, 2B_2023, 3B_2023, HR_2023, BB_2023, SO_2023, P/PA_2023, BA_2023, OBP_2023, SLG_2023, OPS_2023:\n",
    "#so the input has 15 columns, the output has 15 columns:\n",
    "\n",
    "#for x, only keep the rows age_2022, team_2022, pos_2022, PA_2022, AB_2022, H_2022, 2B_2022, 3B_2022, HR_2022, BB_2022, SO_2022, P/PA_2022, BA_2022, OBP_2022, SLG_2022, OPS_2022\n",
    "#and for y, only keep the rows age_2023, team_2023, pos_2023, PA_2023, AB_2023, H_2023, 2B_2023, 3B_2023, HR_2023, BB_2023, SO_2023, P/PA_2023, BA_2023, OBP_2023, SLG_2023, OPS_2023\n",
    "#drop team from df\n",
    "#drop the team_2022 and team_2023 columns:\n",
    "\n",
    "\n",
    "#split the data into X and y\n",
    "X = df[['age_2022', 'PA_2022', 'AB_2022', 'H_2022', '2B_2022', '3B_2022', 'HR_2022', 'BB_2022', 'SO_2022', 'P/PA_2022', 'BA_2022', 'OBP_2022', 'SLG_2022', 'OPS_2022', 'average_distance_traveled_2022']].values\n",
    "y = df[['H_2023']].values\n",
    "\n",
    "# in x we have pos_2022, we need to convert this to a one hot encoding\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "# onehotencoder = OneHotEncoder()\n",
    "# pos_2022 = onehotencoder.fit_transform(df[['pos_2022']]).toarray()\n",
    "# X = np.concatenate((X, pos_2022), axis=1)\n",
    "\n",
    "# #drop the pos_2022 column\n",
    "# X = np.delete(X, 1, axis=1)\n",
    "\n",
    "# #print column names of X\n",
    "\n",
    "\n",
    "\n",
    "#split the data into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#scale the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "y_train = scaler.fit_transform(y_train)\n",
    "y_test = scaler.transform(y_test)\n",
    "\n",
    "#convert the data to tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "#make a dataloader\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "#make the neural network\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(X.shape[1], 64)\n",
    "        self.fc2 = nn.Linear(64, 128)\n",
    "        self.fc3 = nn.Linear(128, y.shape[1])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "model = Net()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.000125)\n",
    "\n",
    "#train the model\n",
    "for epoch in range(100):\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_batch)\n",
    "        loss = criterion(output, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'epoch {epoch+1}, loss: {loss.item()}')\n",
    "\n",
    "#evaluate the model\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X_test)\n",
    "    loss = criterion(y_pred, y_test)\n",
    "    print(f'test loss: {loss.item()}')\n",
    "    print(f'test mse: {mean_squared_error(y_test, y_pred)}')\n",
    "\n",
    "#save the model\n",
    "torch.save(model.state_dict(), 'model.pth')\n",
    "\n",
    "#print r2 of the model\n",
    "from sklearn.metrics import r2_score\n",
    "print(r2_score(y_test, y_pred))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([286.9218]) 157\n",
      "tensor([266.0181]) 168\n",
      "tensor([245.0566]) 162\n",
      "tensor([234.7612]) 160\n",
      "tensor([203.9346]) 144\n",
      "tensor([315.8021]) 147\n",
      "tensor([194.4701]) 122\n",
      "tensor([229.9673]) 100\n",
      "tensor([296.9379]) 139\n",
      "tensor([237.2370]) 101\n",
      "tensor([281.7636]) 97\n",
      "tensor([264.7977]) 129\n",
      "tensor([234.4664]) 96\n",
      "tensor([247.1158]) 161\n",
      "tensor([321.2789]) 204\n",
      "tensor([330.1233]) 193\n",
      "tensor([209.2000]) 112\n",
      "tensor([293.2592]) 212\n",
      "tensor([360.0347]) 158\n",
      "tensor([292.0163]) 107\n",
      "tensor([215.0670]) 95\n",
      "tensor([285.9044]) 135\n",
      "tensor([236.8862]) 164\n",
      "tensor([228.1310]) 150\n",
      "tensor([230.2863]) 189\n",
      "tensor([257.1998]) 188\n",
      "tensor([253.8197]) 138\n",
      "tensor([218.9882]) 143\n",
      "tensor([246.1270]) 157\n",
      "tensor([258.6054]) 149\n",
      "tensor([270.1025]) 185\n",
      "tensor([243.4768]) 187\n",
      "tensor([230.1258]) 192\n",
      "tensor([286.0833]) 151\n",
      "tensor([342.8449]) 156\n",
      "tensor([328.5769]) 163\n",
      "tensor([330.1681]) 118\n",
      "tensor([334.0115]) 142\n",
      "tensor([213.3468]) 161\n",
      "tensor([305.6221]) 149\n",
      "tensor([246.4794]) 160\n",
      "tensor([235.7955]) 172\n",
      "tensor([276.5143]) 164\n",
      "tensor([241.0565]) 142\n",
      "tensor([338.9868]) 109\n",
      "tensor([236.6697]) 164\n",
      "tensor([232.2721]) 154\n",
      "tensor([272.0852]) 117\n",
      "tensor([262.9501]) 162\n",
      "tensor([227.4468]) 153\n",
      "tensor([203.8566]) 154\n",
      "tensor([228.7685]) 214\n",
      "tensor([363.3655]) 211\n",
      "tensor([306.3287]) 199\n",
      "tensor([268.8700]) 172\n",
      "tensor([294.0747]) 180\n",
      "tensor([246.8849]) 139\n",
      "tensor([269.2848]) 196\n",
      "tensor([288.0857]) 123\n",
      "tensor([215.2233]) 147\n",
      "tensor([210.9220]) 119\n",
      "tensor([257.4817]) 156\n",
      "tensor([322.6132]) 129\n",
      "tensor([256.7450]) 191\n",
      "tensor([230.6563]) 230\n",
      "tensor([223.6431]) 120\n",
      "tensor([209.9576]) 174\n",
      "tensor([280.4531]) 175\n",
      "tensor([294.1244]) 141\n",
      "tensor([262.1006]) 163\n",
      "tensor([278.3370]) 143\n",
      "tensor([260.3298]) 170\n",
      "tensor([309.1075]) 171\n",
      "tensor([243.1473]) 200\n",
      "tensor([241.2044]) 143\n",
      "tensor([206.7964]) 109\n",
      "tensor([257.8952]) 132\n",
      "tensor([218.9707]) 121\n",
      "tensor([274.1515]) 139\n",
      "tensor([265.2690]) 251\n",
      "tensor([240.7538]) 183\n",
      "tensor([234.3740]) 188\n",
      "tensor([316.8119]) 174\n",
      "tensor([269.7388]) 168\n",
      "tensor([203.6365]) 122\n",
      "tensor([251.5280]) 132\n",
      "tensor([347.5296]) 181\n",
      "tensor([207.3699]) 145\n",
      "tensor([337.7384]) 177\n",
      "tensor([299.8784]) 139\n",
      "tensor([237.4849]) 131\n",
      "tensor([307.3313]) 178\n",
      "tensor([230.2925]) 177\n",
      "tensor([217.2606]) 153\n",
      "tensor([229.5263]) 117\n",
      "tensor([260.0826]) 166\n",
      "tensor([215.9024]) 200\n",
      "tensor([246.7542]) 152\n",
      "tensor([247.9390]) 153\n",
      "tensor([241.6992]) 173\n",
      "tensor([244.4585]) 89\n",
      "tensor([204.8461]) 112\n",
      "tensor([242.3257]) 198\n",
      "tensor([240.2568]) 167\n",
      "tensor([231.0255]) 145\n",
      "tensor([226.6384]) 131\n",
      "tensor([328.5329]) 122\n",
      "tensor([279.0824]) 117\n",
      "tensor([254.2524]) 150\n",
      "tensor([208.8749]) 147\n",
      "tensor([216.0206]) 165\n",
      "tensor([254.7393]) 205\n",
      "tensor([220.5399]) 140\n",
      "tensor([285.1855]) 163\n",
      "tensor([233.4613]) 163\n",
      "tensor([242.7452]) 204\n",
      "tensor([221.8819]) 147\n",
      "tensor([240.3577]) 130\n",
      "tensor([225.2263]) 108\n",
      "tensor([226.6125]) 98\n",
      "tensor([241.5804]) 194\n",
      "tensor([224.5892]) 145\n",
      "tensor([290.7330]) 173\n",
      "tensor([221.8357]) 96\n",
      "tensor([224.0704]) 154\n",
      "tensor([220.3976]) 123\n",
      "tensor([239.9600]) 135\n",
      "tensor([225.2109]) 130\n",
      "tensor([213.7038]) 158\n",
      "tensor([234.7011]) 170\n",
      "tensor([257.4998]) 158\n",
      "tensor([334.2336]) 188\n",
      "tensor([242.0618]) 164\n",
      "tensor([224.7760]) 140\n",
      "tensor([319.9728]) 127\n",
      "tensor([217.7286]) 114\n",
      "tensor([215.5368]) 124\n",
      "tensor([258.8567]) 206\n",
      "tensor([256.9791]) 157\n",
      "tensor([223.8102]) 132\n",
      "tensor([247.7989]) 160\n",
      "tensor([239.6915]) 161\n",
      "tensor([210.5852]) 120\n",
      "tensor([197.1153]) 139\n",
      "tensor([325.6162]) 162\n",
      "tensor([216.9155]) 158\n",
      "tensor([305.8488]) 168\n",
      "tensor([227.1473]) 138\n",
      "tensor([215.1293]) 172\n",
      "tensor([254.4400]) 158\n",
      "tensor([208.8714]) 139\n",
      "tensor([280.7909]) 145\n",
      "tensor([235.2175]) 172\n",
      "tensor([230.6960]) 122\n",
      "tensor([304.2380]) 91\n",
      "tensor([290.6693]) 112\n",
      "tensor([269.2827]) 195\n",
      "tensor([284.7592]) 123\n",
      "tensor([245.4616]) 196\n",
      "tensor([218.0656]) 219\n",
      "tensor([224.0976]) 135\n",
      "tensor([348.6155]) 193\n",
      "tensor([301.5152]) 163\n",
      "tensor([216.3362]) 146\n",
      "tensor([294.0324]) 125\n",
      "tensor([318.6154]) 147\n",
      "tensor([254.1378]) 107\n",
      "tensor([228.9017]) 129\n",
      "tensor([233.0809]) 182\n",
      "tensor([209.3489]) 193\n",
      "tensor([251.6876]) 171\n",
      "tensor([211.6325]) 170\n",
      "tensor([255.0004]) 170\n",
      "tensor([368.7679]) 225\n",
      "tensor([290.5995]) 148\n",
      "tensor([198.0821]) 164\n",
      "tensor([325.5756]) 192\n",
      "tensor([244.9652]) 217\n",
      "tensor([208.1546]) 126\n",
      "tensor([262.2289]) 184\n",
      "tensor([249.0895]) 172\n",
      "tensor([215.9520]) 149\n",
      "tensor([324.5790]) 174\n",
      "tensor([273.0832]) 182\n",
      "tensor([280.7295]) 114\n",
      "tensor([264.3486]) 192\n",
      "tensor([220.6100]) 160\n",
      "tensor([261.1536]) 131\n",
      "tensor([225.0480]) 158\n",
      "tensor([245.4553]) 168\n",
      "tensor([265.5580]) 115\n",
      "tensor([223.6692]) 149\n",
      "tensor([299.7842]) 105\n",
      "tensor([278.9078]) 147\n",
      "tensor([238.6101]) 136\n",
      "tensor([260.1779]) 197\n",
      "tensor([237.5371]) 192\n",
      "tensor([223.8334]) 147\n",
      "tensor([301.1085]) 148\n",
      "tensor([221.2801]) 171\n",
      "tensor([223.6904]) 134\n",
      "tensor([233.0792]) 172\n",
      "tensor([217.6707]) 117\n",
      "tensor([225.6018]) 112\n",
      "tensor([252.1620]) 134\n",
      "tensor([241.9866]) 184\n",
      "tensor([311.7315]) 156\n",
      "tensor([303.6926]) 137\n",
      "tensor([252.3034]) 195\n",
      "tensor([239.1162]) 136\n",
      "tensor([223.1870]) 236\n",
      "tensor([217.9879]) 130\n",
      "tensor([243.6039]) 191\n",
      "tensor([377.9909]) 212\n",
      "tensor([225.5311]) 148\n",
      "tensor([324.9015]) 186\n",
      "tensor([267.4520]) 121\n",
      "tensor([226.8975]) 215\n",
      "tensor([235.4675]) 213\n",
      "tensor([230.3640]) 163\n",
      "tensor([301.3256]) 134\n",
      "tensor([245.3454]) 132\n",
      "tensor([346.9642]) 183\n",
      "tensor([220.4743]) 88\n",
      "tensor([299.5026]) 193\n",
      "tensor([211.1603]) 197\n",
      "tensor([252.0389]) 157\n",
      "tensor([270.1800]) 183\n",
      "tensor([371.4862]) 175\n",
      "tensor([247.0332]) 166\n",
      "tensor([234.4875]) 158\n",
      "tensor([223.2437]) 154\n",
      "tensor([275.7269]) 128\n",
      "tensor([238.1026]) 162\n",
      "tensor([339.3878]) 176\n",
      "tensor([353.8998]) 147\n",
      "tensor([262.1820]) 153\n",
      "Steve Susdorf tensor([377.9909])\n",
      "Eric Cyr 251\n"
     ]
    }
   ],
   "source": [
    "#use the model to print the top 3 players who are predicted to have the most hits in 2023:\n",
    "#heres the code:\n",
    "\n",
    "#load the model\n",
    "model = Net()\n",
    "model.load_state_dict(torch.load('model.pth'))\n",
    "\n",
    "#iterate through the df:\n",
    "\n",
    "\n",
    "#set new_input to first row, with these columns: ['age_2022',  'PA_2022', 'AB_2022', 'H_2022', '2B_2022', '3B_2022', 'HR_2022', 'BB_2022', 'SO_2022', 'P/PA_2022', 'BA_2022', 'OBP_2022', 'SLG_2022', 'OPS_2022', 'average_distance_traveled_2022']\n",
    "#set new_input to a tensor\n",
    "best_player_name = None\n",
    "max_hits = 0\n",
    "\n",
    "real_best_player_name = None\n",
    "real_max_hits = 0\n",
    "for i in range(len(df)):\n",
    "    new_input = torch.tensor(df[['age_2022', 'PA_2022', 'AB_2022', 'H_2022', '2B_2022', '3B_2022', 'HR_2022', 'BB_2022', 'SO_2022', 'P/PA_2022', 'BA_2022', 'OBP_2022', 'SLG_2022', 'OPS_2022', 'average_distance_traveled_2022']].values[i], dtype=torch.float32)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(new_input)\n",
    "        print(output, df['H_2023'].values[i])\n",
    "        if output > max_hits:\n",
    "            max_hits = output\n",
    "            best_player_name = df['Name'].values[i]\n",
    "        if df['H_2023'].values[i] > real_max_hits:\n",
    "            real_max_hits = df['H_2023'].values[i]\n",
    "            real_best_player_name = df['Name'].values[i]\n",
    "print(best_player_name, max_hits)\n",
    "print(real_best_player_name, real_max_hits)\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
