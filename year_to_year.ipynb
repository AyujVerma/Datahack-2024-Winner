{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#access the csv called batting_season_summary.csv in example_data:\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('example_data/batting_season_summary.csv')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#alrigth so we have the columns of years\n",
    "# i want to rearrange it, such that each row has the player with a data for year x, and then the data for year x+1\n",
    "#for example, if there used to be two rows like:\n",
    "# player1, data1, data2, data3, 2014\n",
    "# player1, data4, data5, data6, 2015\n",
    "#player1, data8, data9, data10, 2016\n",
    "#then this should make 2 rows like:\n",
    "#player1, data1, data2, data3, 2014, data4, data5, data6, 2015\n",
    "#player1, data4, data5, data6, 2015, data8, data9, data10, 2016\n",
    "#anyways, for a particular player who has entries for n years, there should be n-1 rows for that player in the new dataframe.\n",
    "#heres the code:\n",
    "\n",
    "#no more comments, heres the code:\n",
    "\n",
    "#sort the df by player and year\n",
    "df = df.sort_values(by=['Name', 'Year'])\n",
    "#drop the columns 'team' and 'pos' \n",
    "df = df.drop(columns=['team', 'pos'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2700, 16)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss: 26542.6875\n",
      "epoch 2, loss: 20002.330078125\n",
      "epoch 3, loss: 17104.216796875\n",
      "epoch 4, loss: 7737.54833984375\n",
      "epoch 5, loss: 3047.875732421875\n",
      "epoch 6, loss: 2668.345947265625\n",
      "epoch 7, loss: 2135.83154296875\n",
      "epoch 8, loss: 1949.6180419921875\n",
      "epoch 9, loss: 1670.4635009765625\n",
      "epoch 10, loss: 999.3551635742188\n",
      "epoch 11, loss: 1261.3460693359375\n",
      "epoch 12, loss: 1081.0579833984375\n",
      "epoch 13, loss: 683.124755859375\n",
      "epoch 14, loss: 757.15234375\n",
      "epoch 15, loss: 798.0426635742188\n",
      "epoch 16, loss: 473.7430419921875\n",
      "epoch 17, loss: 562.0117797851562\n",
      "epoch 18, loss: 484.504638671875\n",
      "epoch 19, loss: 418.5822448730469\n",
      "epoch 20, loss: 352.607666015625\n",
      "epoch 21, loss: 307.84930419921875\n",
      "epoch 22, loss: 534.7980346679688\n",
      "epoch 23, loss: 288.90997314453125\n",
      "epoch 24, loss: 227.56394958496094\n",
      "epoch 25, loss: 184.62200927734375\n",
      "epoch 26, loss: 192.52256774902344\n",
      "epoch 27, loss: 102.4846420288086\n",
      "epoch 28, loss: 117.33736419677734\n",
      "epoch 29, loss: 113.6229248046875\n",
      "epoch 30, loss: 95.54291534423828\n",
      "epoch 31, loss: 78.1455307006836\n",
      "epoch 32, loss: 83.4542007446289\n",
      "epoch 33, loss: 61.404483795166016\n",
      "epoch 34, loss: 38.51791763305664\n",
      "epoch 35, loss: 51.98221969604492\n",
      "epoch 36, loss: 23.039403915405273\n",
      "epoch 37, loss: 27.367753982543945\n",
      "epoch 38, loss: 29.66045379638672\n",
      "epoch 39, loss: 20.398195266723633\n",
      "epoch 40, loss: 27.911672592163086\n",
      "epoch 41, loss: 18.722728729248047\n",
      "epoch 42, loss: 23.04529571533203\n",
      "epoch 43, loss: 11.10903263092041\n",
      "epoch 44, loss: 10.10716724395752\n",
      "epoch 45, loss: 9.85430908203125\n",
      "epoch 46, loss: 9.498262405395508\n",
      "epoch 47, loss: 11.640037536621094\n",
      "epoch 48, loss: 7.234426975250244\n",
      "epoch 49, loss: 11.65857982635498\n",
      "epoch 50, loss: 7.885725498199463\n",
      "epoch 51, loss: 5.23107385635376\n",
      "epoch 52, loss: 7.838160037994385\n",
      "epoch 53, loss: 7.616708278656006\n",
      "epoch 54, loss: 4.632968425750732\n",
      "epoch 55, loss: 4.642829895019531\n",
      "epoch 56, loss: 3.282402992248535\n",
      "epoch 57, loss: 3.4180843830108643\n",
      "epoch 58, loss: 2.407383918762207\n",
      "epoch 59, loss: 3.567957878112793\n",
      "epoch 60, loss: 5.0398478507995605\n",
      "epoch 61, loss: 3.343388795852661\n",
      "epoch 62, loss: 4.202654838562012\n",
      "epoch 63, loss: 3.4617748260498047\n",
      "epoch 64, loss: 3.262465238571167\n",
      "epoch 65, loss: 2.870694160461426\n",
      "epoch 66, loss: 2.904369354248047\n",
      "epoch 67, loss: 2.2782866954803467\n",
      "epoch 68, loss: 3.3347063064575195\n",
      "epoch 69, loss: 1.9524298906326294\n",
      "epoch 70, loss: 4.076673984527588\n",
      "epoch 71, loss: 2.8147823810577393\n",
      "epoch 72, loss: 4.113796234130859\n",
      "epoch 73, loss: 1.6338266134262085\n",
      "epoch 74, loss: 3.119847536087036\n",
      "epoch 75, loss: 1.5312894582748413\n",
      "epoch 76, loss: 2.718191385269165\n",
      "epoch 77, loss: 1.646399974822998\n",
      "epoch 78, loss: 1.926332950592041\n",
      "epoch 79, loss: 1.514414668083191\n",
      "epoch 80, loss: 1.2894048690795898\n",
      "epoch 81, loss: 3.0312998294830322\n",
      "epoch 82, loss: 3.1577072143554688\n",
      "epoch 83, loss: 1.0823489427566528\n",
      "epoch 84, loss: 2.0418617725372314\n",
      "epoch 85, loss: 2.257326602935791\n",
      "epoch 86, loss: 1.422402262687683\n",
      "epoch 87, loss: 1.2759180068969727\n",
      "epoch 88, loss: 1.4387969970703125\n",
      "epoch 89, loss: 1.4697657823562622\n",
      "epoch 90, loss: 1.3017715215682983\n",
      "epoch 91, loss: 1.1208217144012451\n",
      "epoch 92, loss: 1.0922402143478394\n",
      "epoch 93, loss: 1.3384919166564941\n",
      "epoch 94, loss: 1.2864065170288086\n",
      "epoch 95, loss: 0.7326352596282959\n",
      "epoch 96, loss: 0.5961626172065735\n",
      "epoch 97, loss: 0.8819822669029236\n",
      "epoch 98, loss: 1.4925671815872192\n",
      "epoch 99, loss: 1.3350672721862793\n",
      "epoch 100, loss: 1.1684187650680542\n",
      "test loss: 1.096006155014038\n",
      "test mse: 1.096006155014038\n"
     ]
    }
   ],
   "source": [
    "#alright do a neural network taking all this data (except name and year) and predict H. use pytorch:\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "#for reference theres 14 columns in the df, 1 of them is the target (H). everything has been dropped so just use. heres the code:\n",
    "\n",
    "#split the data into train and test\n",
    "X = df.drop(columns=['H', 'BA'] ).values\n",
    "y = df['H'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#scale the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "#convert the data to tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "#make a dataloader\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "#make the neural network\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(X.shape[1], 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "model = Net()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "#train the model\n",
    "for epoch in range(100):\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_batch)\n",
    "        loss = criterion(output, y_batch.view(-1, 1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'epoch {epoch+1}, loss: {loss.item()}')\n",
    "\n",
    "#evaluate the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X_test)\n",
    "    loss = criterion(y_pred, y_test.view(-1, 1))\n",
    "    print(f'test loss: {loss.item()}')\n",
    "    print(f'test mse: {mean_squared_error(y_test, y_pred)}')\n",
    "\n",
    "#save the model\n",
    "torch.save(model.state_dict(), 'model.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9987925900738972\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 8, 'n_estimators': 70}\n",
      "0.9669703256409132\n",
      "36.785861681433786\n",
      "0.9594750342526668\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from math import radians, sin, cos, sqrt, atan2\n",
    "\n",
    "# Function to calculate distance between two geographical coordinates using Haversine formula\n",
    "def calculate_distance(lat1, lon1, lat2, lon2):\n",
    "    # Radius of the Earth in km\n",
    "    R = 6371.0\n",
    "    \n",
    "    # Convert latitude and longitude from degrees to radians\n",
    "    lat1 = radians(lat1)\n",
    "    lon1 = radians(lon1)\n",
    "    lat2 = radians(lat2)\n",
    "    lon2 = radians(lon2)\n",
    "    \n",
    "    # Calculate the change in coordinates\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "    \n",
    "    # Apply Haversine formula\n",
    "    a = sin(dlat / 2)**2 + cos(lat1) * cos(lat2) * sin(dlon / 2)**2\n",
    "    c = 2 * atan2(sqrt(a), sqrt(1 - a))\n",
    "    distance = R * c\n",
    "    \n",
    "    return distance\n",
    "\n",
    "# Function to get average distance traveled for a given year and team\n",
    "def get_average_distance(year, team):\n",
    "    # Load schedule data for the given year\n",
    "    schedule_filename = f'example_data/schedules/{year}_schedule.csv'\n",
    "    total_distance = 0\n",
    "    games_played = 0\n",
    "    \n",
    "    # Parse the schedule and calculate distances\n",
    "    with open(schedule_filename, 'r') as file:\n",
    "        next(file)  # Skip header\n",
    "        for line in file:\n",
    "            schedule_year, _, home_team, away_team = line.strip().split(',')[0:4]\n",
    "            \n",
    "            # Check if it's an away game for the given team\n",
    "            if schedule_year == year and away_team == team:\n",
    "                # Check if coordinates for both cities are available\n",
    "                if home_team in city_coordinates and away_team in city_coordinates:\n",
    "                    home_lat, home_lon = city_coordinates[home_team]\n",
    "                    away_lat, away_lon = city_coordinates[away_team]\n",
    "                    \n",
    "                    # Calculate distance between home and away cities\n",
    "                    distance = calculate_distance(home_lat, home_lon, away_lat, away_lon)\n",
    "                    total_distance += distance\n",
    "                    games_played += 1\n",
    "    \n",
    "    # Calculate average distance traveled\n",
    "    if games_played > 0:\n",
    "        return total_distance / games_played\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Dictionary to store the coordinates of each city\n",
    "city_coordinates = {\n",
    "    'Royals': (39.0513, -94.4805),     # Kansas City (Royals)\n",
    "    'Braves': (33.8908, -84.4679),     # Atlanta (Braves)\n",
    "    'Rays': (27.7684, -82.6483),       # St. Petersburg (Rays)\n",
    "    'Blue Jays': (43.6414, -79.3894),  # Toronto (Blue Jays)\n",
    "    'Diamondbacks': (33.4455, -112.0667),  # Phoenix (Diamondbacks)\n",
    "    'Astros': (29.7572, -95.3554),     # Houston (Astros)\n",
    "    'Pirates': (40.4469, -80.0057),    # Pittsburgh (Pirates)\n",
    "    'Dodgers': (34.0736, -118.2400),   # Los Angeles (Dodgers)\n",
    "    'Rockies': (39.7554, -104.9881),   # Denver (Rockies)\n",
    "    'Nationals': (38.8729, -77.0074),  # Washington D.C. (Nationals)\n",
    "    'Cardinals': (38.6226, -90.1928),  # St. Louis (Cardinals)\n",
    "    'Red Sox': (42.3467, -71.0972),    # Boston (Red Sox)\n",
    "    'Orioles': (39.2839, -76.6217),    # Baltimore (Orioles)\n",
    "    'Giants': (37.7786, -122.3893),    # San Francisco (Giants)\n",
    "    'Reds': (39.0979, -84.5086),       # Cincinnati (Reds)\n",
    "    'Indians': (41.4959, -81.6853),    # Cleveland (Indians)\n",
    "    'Padres': (32.7076, -117.1570),    # San Diego (Padres)\n",
    "    'Phillies': (39.9054, -75.1669),   # Philadelphia (Phillies)\n",
    "    'White Sox': (41.8301, -87.6347),  # Chicago (White Sox)\n",
    "    'Brewers': (43.0280, -87.9712),    # Milwaukee (Brewers)\n",
    "    'Yankees': (40.8296, -73.9262),    # New York City (Yankees)\n",
    "    'Mets': (40.7571, -73.8458),       # New York City (Mets)\n",
    "    'Rangers': (32.7511, -97.0820),    # Arlington (Rangers)\n",
    "    'Marlins': (25.7780, -80.2195),    # Miami (Marlins)\n",
    "    'Mariners': (47.5914, -122.3325),  # Seattle (Mariners)\n",
    "    'Twins': (44.9817, -93.2784),      # Minneapolis (Twins)\n",
    "    'Angels': (33.8003, -117.8827),    # Anaheim (Angels)\n",
    "    'Cubs': (41.9484, -87.6553),       # Chicago (Cubs)\n",
    "    'Athletics': (37.7516, -122.2005), # Oakland (Athletics)\n",
    "    'Tigers': (42.3391, -83.0487)      # Detroit (Tigers\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Name  age_2021  age_2022  age_2023     team_2021  \\\n",
      "0         Aaron Looper        36        37        38  Diamondbacks   \n",
      "1       Aaron Scheffer        37        38        39        Astros   \n",
      "2        Adam Peterson        28        29        30        Braves   \n",
      "3        Adrian Houser        40        41        42        Royals   \n",
      "4      Alberto Cabrera        24        25        26        Angels   \n",
      "..                 ...       ...       ...       ...           ...   \n",
      "206    Tyler Ladendorf        28        29        30       Rangers   \n",
      "207      Victor Garate        27        28        29      Phillies   \n",
      "208      Vince Belnome        29        30        31        Giants   \n",
      "209     Wilfredo Tovar        25        26        27      Mariners   \n",
      "210  Wilking Rodriguez        32        33        34         Twins   \n",
      "\n",
      "        team_2022     team_2023 pos_2021 pos_2022 pos_2023  ...  OBP_2023  \\\n",
      "0    Diamondbacks  Diamondbacks        C        C        C  ...  0.327801   \n",
      "1          Astros        Astros        C        C        C  ...  0.315436   \n",
      "2          Braves        Braves       1B       1B       1B  ...  0.310830   \n",
      "3          Royals        Royals       SS       SS       SS  ...  0.287267   \n",
      "4          Angels        Angels       LF       LF       LF  ...  0.361702   \n",
      "..            ...           ...      ...      ...      ...  ...       ...   \n",
      "206       Rangers       Rangers       1B       1B       1B  ...  0.293878   \n",
      "207      Phillies      Phillies       RF       RF       RF  ...  0.349845   \n",
      "208        Giants        Giants       SS       SS       SS  ...  0.302817   \n",
      "209      Mariners      Mariners       1B       1B       1B  ...  0.304233   \n",
      "210         Twins         Twins       2B       2B       2B  ...  0.305516   \n",
      "\n",
      "     SLG_2021  SLG_2022  SLG_2023  OPS_2021  OPS_2022  OPS_2023  \\\n",
      "0    0.399404  0.421374  0.419907  0.688262  0.740876  0.747708   \n",
      "1    0.438336  0.500000  0.412979  0.763029  0.836957  0.728416   \n",
      "2    0.442943  0.432974  0.457055  0.768556  0.766307  0.767885   \n",
      "3    0.323988  0.364548  0.426202  0.549213  0.631215  0.713469   \n",
      "4    0.372617  0.432709  0.402116  0.673086  0.777588  0.763819   \n",
      "..        ...       ...       ...       ...       ...       ...   \n",
      "206  0.317337  0.329771  0.319938  0.619505  0.632474  0.613816   \n",
      "207  0.473958  0.504967  0.486254  0.815651  0.872283  0.836099   \n",
      "208  0.449165  0.442040  0.469449  0.765405  0.751393  0.772265   \n",
      "209  0.299392  0.332834  0.349183  0.598197  0.631638  0.653416   \n",
      "210  0.468553  0.409449  0.414596  0.814492  0.739042  0.720113   \n",
      "\n",
      "     average_distance_traveled_2021  average_distance_traveled_2022  \\\n",
      "0                       2246.437903                     2225.270942   \n",
      "1                       1799.509611                     1804.150154   \n",
      "2                       1557.452457                     1579.628997   \n",
      "3                       1435.973088                     1392.793060   \n",
      "4                       2773.574581                     2530.414662   \n",
      "..                              ...                             ...   \n",
      "206                     1602.696589                     1670.087164   \n",
      "207                     1981.307272                     1666.534873   \n",
      "208                     2644.084883                     2917.531625   \n",
      "209                     3130.918615                     2815.169960   \n",
      "210                     1520.461855                     1625.335139   \n",
      "\n",
      "     average_distance_traveled_2023  \n",
      "0                       2179.360569  \n",
      "1                       1851.571809  \n",
      "2                       1555.648618  \n",
      "3                       1393.296270  \n",
      "4                       2629.145853  \n",
      "..                              ...  \n",
      "206                     1653.825187  \n",
      "207                     1760.432996  \n",
      "208                     2737.547001  \n",
      "209                     2918.115753  \n",
      "210                     1584.117562  \n",
      "\n",
      "[211 rows x 52 columns]\n"
     ]
    }
   ],
   "source": [
    "#ok this is what we're going to do. look ath the file batting_season_summary.\n",
    "# Name,age,team,pos,PA,AB,H,2B,3B,HR,BB,SO,P/PA,BA,OBP,SLG,OPS,Year.\n",
    "\n",
    "#train a neural networking taking age, team, pos, PA, AB, 2B, 3B, HR, BB, SO, P/PA, BA, OBP, SLG, OPS, SPECIFICALLY FOR YEAR 2022 as input,\n",
    "#and outputs age, team, pos, PA, AB, H, 2B, 3B, HR, BB, SO, P/PA, BA, OBP, SLG, OPS for year 2023.\n",
    "#basically each player who has had an entry in both 2022 and 2023 will be sort of used as data, where the 2022 one is their input and the 2023 one is their output to compare against.\n",
    "#so let's make the model:\n",
    "\n",
    "#first we need to get the data for 2022 and 2023\n",
    "df = pd.read_csv('example_data/batting_season_summary.csv')\n",
    "df['average_distance_traveled'] = df.apply(lambda row: get_average_distance(str(row['Year']), row['team']), axis=1)\n",
    "\n",
    "#drop the rows that don't have 2022 or 2023\n",
    "df = df[ (df['Year'] == 2021) | (df['Year'] == 2022) | (df['Year'] == 2023) ]\n",
    "#sort by name:\n",
    "df = df.sort_values(by='Name')\n",
    "\n",
    "#drop names that only have 1 entry\n",
    "df = df.groupby('Name').filter(lambda x: len(x) == 3)\n",
    "\n",
    "#now basically combine the 2022 and 2023 data for each player into one row:\n",
    "df = df.pivot(index='Name', columns='Year')\n",
    "df.columns = ['_'.join(map(str, col)).strip() for col in df.columns.values]\n",
    "df = df.reset_index()\n",
    "print(df) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment = pd.read_csv('example_data/sentinment.csv')\n",
    "df = df.merge(sentiment, on='Name', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Name', 'age_2021', 'age_2022', 'age_2023', 'team_2021', 'team_2022',\n",
      "       'team_2023', 'pos_2021', 'pos_2022', 'pos_2023', 'PA_2021', 'PA_2022',\n",
      "       'PA_2023', 'AB_2021', 'AB_2022', 'AB_2023', 'H_2021', 'H_2022',\n",
      "       'H_2023', '2B_2021', '2B_2022', '2B_2023', '3B_2021', '3B_2022',\n",
      "       '3B_2023', 'HR_2021', 'HR_2022', 'HR_2023', 'BB_2021', 'BB_2022',\n",
      "       'BB_2023', 'SO_2021', 'SO_2022', 'SO_2023', 'P/PA_2021', 'P/PA_2022',\n",
      "       'P/PA_2023', 'BA_2021', 'BA_2022', 'BA_2023', 'OBP_2021', 'OBP_2022',\n",
      "       'OBP_2023', 'SLG_2021', 'SLG_2022', 'SLG_2023', 'OPS_2021', 'OPS_2022',\n",
      "       'OPS_2023', 'average_distance_traveled_2021',\n",
      "       'average_distance_traveled_2022', 'average_distance_traveled_2023',\n",
      "       'Score'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss: 1.3334726095199585\n",
      "epoch 2, loss: 1.135761022567749\n",
      "epoch 3, loss: 1.170819640159607\n",
      "epoch 4, loss: 0.9441331028938293\n",
      "epoch 5, loss: 0.8970918655395508\n",
      "epoch 6, loss: 0.8626011610031128\n",
      "epoch 7, loss: 0.9614067077636719\n",
      "epoch 8, loss: 0.9354349374771118\n",
      "epoch 9, loss: 0.6928685903549194\n",
      "epoch 10, loss: 1.0630168914794922\n",
      "epoch 11, loss: 0.9750477075576782\n",
      "epoch 12, loss: 0.8370993733406067\n",
      "epoch 13, loss: 0.8857852816581726\n",
      "epoch 14, loss: 0.8058411478996277\n",
      "epoch 15, loss: 1.0075128078460693\n",
      "epoch 16, loss: 0.5409210324287415\n",
      "epoch 17, loss: 0.6320751905441284\n",
      "epoch 18, loss: 0.7550197839736938\n",
      "epoch 19, loss: 0.6501318216323853\n",
      "epoch 20, loss: 0.7268966436386108\n",
      "epoch 21, loss: 0.7586232423782349\n",
      "epoch 22, loss: 0.5833332538604736\n",
      "epoch 23, loss: 0.6707878112792969\n",
      "epoch 24, loss: 0.6366502046585083\n",
      "epoch 25, loss: 0.6785306930541992\n",
      "epoch 26, loss: 0.5667264461517334\n",
      "epoch 27, loss: 0.5403729677200317\n",
      "epoch 28, loss: 0.5060898065567017\n",
      "epoch 29, loss: 0.4906318187713623\n",
      "epoch 30, loss: 0.4613686501979828\n",
      "epoch 31, loss: 0.48593980073928833\n",
      "epoch 32, loss: 0.5371734499931335\n",
      "epoch 33, loss: 0.4603317677974701\n",
      "epoch 34, loss: 0.4201164245605469\n",
      "epoch 35, loss: 0.5143753290176392\n",
      "epoch 36, loss: 0.380317360162735\n",
      "epoch 37, loss: 0.3496960997581482\n",
      "epoch 38, loss: 0.35892951488494873\n",
      "epoch 39, loss: 0.2805742621421814\n",
      "epoch 40, loss: 0.31677842140197754\n",
      "epoch 41, loss: 0.2864539921283722\n",
      "epoch 42, loss: 0.27081498503685\n",
      "epoch 43, loss: 0.25054970383644104\n",
      "epoch 44, loss: 0.21341340243816376\n",
      "epoch 45, loss: 0.26211363077163696\n",
      "epoch 46, loss: 0.23273876309394836\n",
      "epoch 47, loss: 0.21677689254283905\n",
      "epoch 48, loss: 0.1463223397731781\n",
      "epoch 49, loss: 0.22448274493217468\n",
      "epoch 50, loss: 0.2035323828458786\n",
      "epoch 51, loss: 0.24811697006225586\n",
      "epoch 52, loss: 0.19994935393333435\n",
      "epoch 53, loss: 0.22021183371543884\n",
      "epoch 54, loss: 0.17417246103286743\n",
      "epoch 55, loss: 0.18447288870811462\n",
      "epoch 56, loss: 0.19092819094657898\n",
      "epoch 57, loss: 0.14350058138370514\n",
      "epoch 58, loss: 0.1735428273677826\n",
      "epoch 59, loss: 0.15696707367897034\n",
      "epoch 60, loss: 0.13389702141284943\n",
      "epoch 61, loss: 0.1830795556306839\n",
      "epoch 62, loss: 0.13158269226551056\n",
      "epoch 63, loss: 0.19372329115867615\n",
      "epoch 64, loss: 0.18467257916927338\n",
      "epoch 65, loss: 0.12202610075473785\n",
      "epoch 66, loss: 0.13837967813014984\n",
      "epoch 67, loss: 0.14521679282188416\n",
      "epoch 68, loss: 0.1898730993270874\n",
      "epoch 69, loss: 0.14792048931121826\n",
      "epoch 70, loss: 0.14570608735084534\n",
      "epoch 71, loss: 0.13663896918296814\n",
      "epoch 72, loss: 0.13834461569786072\n",
      "epoch 73, loss: 0.11400029808282852\n",
      "epoch 74, loss: 0.14887520670890808\n",
      "epoch 75, loss: 0.10473759472370148\n",
      "epoch 76, loss: 0.11826028674840927\n",
      "epoch 77, loss: 0.12463410943746567\n",
      "epoch 78, loss: 0.09847350418567657\n",
      "epoch 79, loss: 0.14582501351833344\n",
      "epoch 80, loss: 0.1104021668434143\n",
      "epoch 81, loss: 0.11461801826953888\n",
      "epoch 82, loss: 0.15525200963020325\n",
      "epoch 83, loss: 0.14950311183929443\n",
      "epoch 84, loss: 0.11542432010173798\n",
      "epoch 85, loss: 0.1500251740217209\n",
      "epoch 86, loss: 0.12269767373800278\n",
      "epoch 87, loss: 0.12700317800045013\n",
      "epoch 88, loss: 0.11310206353664398\n",
      "epoch 89, loss: 0.11574892699718475\n",
      "epoch 90, loss: 0.11393757164478302\n",
      "epoch 91, loss: 0.1042884811758995\n",
      "epoch 92, loss: 0.13212743401527405\n",
      "epoch 93, loss: 0.13682061433792114\n",
      "epoch 94, loss: 0.12670114636421204\n",
      "epoch 95, loss: 0.11371209472417831\n",
      "epoch 96, loss: 0.14806203544139862\n",
      "epoch 97, loss: 0.1060807928442955\n",
      "epoch 98, loss: 0.10112731158733368\n",
      "epoch 99, loss: 0.11955468356609344\n",
      "epoch 100, loss: 0.10698197036981583\n",
      "test loss: 0.16441839933395386\n",
      "test mse: 0.16441841423511505\n",
      "0.8765818022196594\n"
     ]
    }
   ],
   "source": [
    "#ok now we train a neural network to predict the 2023 data from the 2022 data:\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "#the input is the 2022 data, the output is the 2023 data.\n",
    "#aka the input is age_2022, team_2022, pos_2022, PA_2022, AB_2022, H_2022, 2B_2022, 3B_2022, HR_2022, BB_2022, SO_2022, P/PA_2022, BA_2022, OBP_2022, SLG_2022, OPS_2022\n",
    "#the output is age_2023, team_2023, pos_2023, PA_2023, AB_2023, H_2023, 2B_2023, 3B_2023, HR_2023, BB_2023, SO_2023, P/PA_2023, BA_2023, OBP_2023, SLG_2023, OPS_2023:\n",
    "#so the input has 15 columns, the output has 15 columns:\n",
    "\n",
    "#for x, only keep the rows age_2022, team_2022, pos_2022, PA_2022, AB_2022, H_2022, 2B_2022, 3B_2022, HR_2022, BB_2022, SO_2022, P/PA_2022, BA_2022, OBP_2022, SLG_2022, OPS_2022\n",
    "#and for y, only keep the rows age_2023, team_2023, pos_2023, PA_2023, AB_2023, H_2023, 2B_2023, 3B_2023, HR_2023, BB_2023, SO_2023, P/PA_2023, BA_2023, OBP_2023, SLG_2023, OPS_2023\n",
    "#drop team from df\n",
    "#drop the team_2022 and team_2023 columns:\n",
    "\n",
    "\n",
    "\n",
    "#split the data into X and y\n",
    "X_22 = df[['age_2022', 'PA_2022', 'AB_2022', 'H_2022', '2B_2022', '3B_2022', 'HR_2022', 'BB_2022', 'SO_2022', 'P/PA_2022', 'BA_2022', 'OBP_2022', 'SLG_2022', 'OPS_2022', 'average_distance_traveled_2022', 'Score']].values\n",
    "y_23 = df[['H_2023']].values\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#now that we have 2021 data, basically for X we can do \n",
    "#X = df[['age_2022', 'PA_2022', 'AB_2022', 'H_2022', '2B_2022', '3B_2022', 'HR_2022', 'BB_2022', 'SO_2022', 'P/PA_2022', 'BA_2022', 'OBP_2022', 'SLG_2022', 'OPS_2022', 'average_distance_traveled_2022']].values\n",
    "#with y being df[['H_2023']].values\n",
    "#AND WE CAN ALSO DO, using 2021,\n",
    "X_21 = df[['age_2021', 'PA_2021', 'AB_2021', 'H_2021', '2B_2021', '3B_2021', 'HR_2021', 'BB_2021', 'SO_2021', 'P/PA_2021', 'BA_2021', 'OBP_2021', 'SLG_2021', 'OPS_2021', 'average_distance_traveled_2021', 'Score']].values\n",
    "y_22 = df[['H_2022']].values\n",
    "\n",
    "#split the data into train and test\n",
    "X = np.concatenate((X_21, X_22), axis=1)\n",
    "y = np.concatenate((y_22, y_23), axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#split the data into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#scale the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "y_train = scaler.fit_transform(y_train)\n",
    "y_test = scaler.transform(y_test)\n",
    "\n",
    "#convert the data to tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "#make a dataloader\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "#make the neural network\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(X.shape[1], 64)\n",
    "        self.fc2 = nn.Linear(64, 128)\n",
    "        self.fc3 = nn.Linear(128, y.shape[1])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "model = Net()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.000125)\n",
    "\n",
    "#train the model\n",
    "for epoch in range(100):\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_batch)\n",
    "        loss = criterion(output, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'epoch {epoch+1}, loss: {loss.item()}')\n",
    "\n",
    "#evaluate the model\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X_test)\n",
    "    loss = criterion(y_pred, y_test)\n",
    "    print(f'test loss: {loss.item()}')\n",
    "    print(f'test mse: {mean_squared_error(y_test, y_pred)}')\n",
    "\n",
    "#save the model\n",
    "torch.save(model.state_dict(), 'model.pth')\n",
    "\n",
    "#print r2 of the model\n",
    "from sklearn.metrics import r2_score\n",
    "print(r2_score(y_test, y_pred))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.60000000e+01 7.27000000e+02 6.71000000e+02 1.54000000e+02\n",
      " 3.10000000e+01 2.50000000e+01 1.10000000e+01 5.60000000e+01\n",
      " 1.54000000e+02 3.50618982e+00 2.29508197e-01 2.88858322e-01\n",
      " 3.99403875e-01 6.88262197e-01 2.24643790e+03 2.99921077e+00\n",
      " 3.70000000e+01 7.23000000e+02 6.55000000e+02 1.63000000e+02\n",
      " 4.20000000e+01 2.20000000e+01 9.00000000e+00 6.80000000e+01\n",
      " 1.38000000e+02 3.42185339e+00 2.48854962e-01 3.19502075e-01\n",
      " 4.21374046e-01 7.40876120e-01 2.22527094e+03 2.99921077e+00]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Name  predicted_hits_2023\n",
      "154   Osiris Matos          9915.171875\n",
      "16     Blake Lalli          9766.781250\n",
      "48   Chris Rearick          9499.338867\n"
     ]
    }
   ],
   "source": [
    "#use the model to print the top 3 players who are predicted to have the most hits in 2023:\n",
    "#heres the code:\n",
    "model.eval()\n",
    "\n",
    "\n",
    "\n",
    "#iterate through the df, print the top 3 players:\n",
    "\n",
    "hits = []\n",
    "for i in range(df.shape[0]):\n",
    "    new_input = torch.tensor(df[['age_2022', 'PA_2022', 'AB_2022', 'H_2022', '2B_2022', '3B_2022', 'HR_2022', 'BB_2022', 'SO_2022', 'P/PA_2022', 'BA_2022', 'OBP_2022', 'SLG_2022', 'OPS_2022', 'average_distance_traveled_2022', 'Score', 'age_2023', 'PA_2023', 'AB_2023', 'H_2023', '2B_2023', '3B_2023', 'HR_2023', 'BB_2023', 'SO_2023', 'P/PA_2023', 'BA_2023', 'OBP_2023', 'SLG_2023', 'OPS_2023', 'average_distance_traveled_2023', 'Score']].values[i], dtype=torch.float32)\n",
    "    #reshape -1, 1\n",
    "    new_input = new_input.view(1, -1)\n",
    "    #append the 2021 data to the input:\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(new_input)\n",
    "        hits.append(scaler.inverse_transform(output.numpy())[0][0])\n",
    "        # print(df['Name'].values[i], scaler.inverse_transform(output.numpy()))\n",
    "\n",
    "#sort the hits\n",
    "df['predicted_hits_2023'] = hits\n",
    "df = df.sort_values(by='predicted_hits_2023', ascending=False)\n",
    "print(df[['Name', 'predicted_hits_2023']].head(3))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
